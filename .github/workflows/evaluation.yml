name: Evaluation Pipeline

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'tests/**'
      - '.github/workflows/evaluation.yml'
  pull_request:
    branches: [main]
    paths:
      - 'src/**'
      - 'tests/**'
      - '.github/workflows/evaluation.yml'
  workflow_dispatch:
    inputs:
      samples:
        description: 'Comma-separated list of sample IDs to evaluate (empty for all)'
        required: false
        default: ''
      threshold_overall:
        description: 'Overall quality threshold (0.0-1.0)'
        required: false
        default: '0.75'

permissions:
  contents: read
  pull-requests: write

env:
  PYTHON_VERSION: '3.12'
  UV_VERSION: '0.5.x'

jobs:
  evaluate:
    name: Run Evaluation
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: uv sync --all-extras --dev

      - name: Run tests first
        run: uv run pytest tests/ -v --tb=short -x

      - name: Run evaluation
        id: eval
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          LANGFUSE_PUBLIC_KEY: ${{ secrets.LANGFUSE_PUBLIC_KEY }}
          LANGFUSE_SECRET_KEY: ${{ secrets.LANGFUSE_SECRET_KEY }}
        run: |
          # Prepare arguments
          ARGS=""
          if [ -n "${{ github.event.inputs.samples }}" ]; then
            ARGS="$ARGS --samples ${{ github.event.inputs.samples }}"
          fi

          THRESHOLD="${{ github.event.inputs.threshold_overall }}"
          if [ -n "$THRESHOLD" ]; then
            ARGS="$ARGS --threshold overall=$THRESHOLD"
          fi

          # Run evaluation
          mkdir -p reports
          uv run python -m src.evaluation \
            --output reports/ \
            --format both \
            --fail-on-regression \
            $ARGS \
            2>&1 | tee evaluation.log

          # Set outputs for PR comment
          echo "passed=$(jq '.passed' reports/eval_*.json)" >> $GITHUB_OUTPUT
          echo "failed=$(jq '.failed' reports/eval_*.json)" >> $GITHUB_OUTPUT
          echo "total=$(jq '.total_samples' reports/eval_*.json)" >> $GITHUB_OUTPUT
          echo "overall=$(jq '.aggregate_metrics.overall' reports/eval_*.json)" >> $GITHUB_OUTPUT

      - name: Upload evaluation results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: evaluation-results
          path: reports/
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Read markdown report
            const reportFiles = fs.readdirSync('reports').filter(f => f.endsWith('.md'));
            let reportContent = '';

            if (reportFiles.length > 0) {
              reportContent = fs.readFileSync(`reports/${reportFiles[0]}`, 'utf8');
            }

            // Create comment body
            const body = `## üîç Evaluation Results

            | Metric | Value |
            |--------|-------|
            | Total Samples | ${{ steps.eval.outputs.total }} |
            | Passed | ${{ steps.eval.outputs.passed }} |
            | Failed | ${{ steps.eval.outputs.failed }} |
            | Overall Score | ${{ steps.eval.outputs.overall }} |

            <details>
            <summary>üìä Full Report</summary>

            ${reportContent}

            </details>

            ---
            *Generated by [Evaluation Pipeline](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})*
            `;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' &&
              c.body.includes('Evaluation Results')
            );

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body,
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body,
              });
            }

      - name: Check quality thresholds
        if: always()
        run: |
          # Read results
          OVERALL=$(jq '.aggregate_metrics.overall' reports/eval_*.json 2>/dev/null || echo "0")
          THRESHOLD="${{ github.event.inputs.threshold_overall || '0.75' }}"

          echo "Overall score: $OVERALL"
          echo "Threshold: $THRESHOLD"

          # Compare using bc for floating point
          if [ "$(echo "$OVERALL < $THRESHOLD" | bc -l)" -eq 1 ]; then
            echo "‚ùå Overall score ($OVERALL) is below threshold ($THRESHOLD)"
            exit 1
          else
            echo "‚úÖ Quality thresholds met"
          fi

  store-baseline:
    name: Store Baseline Results
    runs-on: ubuntu-latest
    needs: evaluate
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Download evaluation results
        uses: actions/download-artifact@v4
        with:
          name: evaluation-results
          path: reports/

      - name: Store as baseline
        uses: actions/upload-artifact@v4
        with:
          name: baseline-results
          path: reports/
          retention-days: 90
