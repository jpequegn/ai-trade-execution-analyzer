"""Evaluation metrics and pipelines for AI analysis quality."""

from src.evaluation.ground_truth import (
    AnnotationMetadata,
    AnnotatorConfidence,
    ExpertAnalysis,
    GroundTruthDataset,
    GroundTruthSample,
    IssueCategory,
    IssueSeverity,
    create_sample,
    load_ground_truth,
    save_ground_truth,
)
from src.evaluation.matching import (
    MatchResult,
    best_match,
    count_matches,
    find_all_matches,
    fuzzy_match,
    keyword_overlap,
    sequence_similarity,
)
from src.evaluation.metrics import (
    DEFAULT_MATCH_THRESHOLD,
    DEFAULT_WEIGHTS,
    EvaluationResult,
    MetricResult,
    completeness,
    completeness_detailed,
    evaluate,
    evaluate_batch,
    factual_correctness,
    factual_correctness_detailed,
    insight_accuracy,
    insight_accuracy_detailed,
    overall_quality,
    score_accuracy,
    score_accuracy_detailed,
)
from src.evaluation.validator import (
    ValidationError,
    ValidationResult,
    validate_dataset,
)

__all__ = [
    "DEFAULT_MATCH_THRESHOLD",
    "DEFAULT_WEIGHTS",
    "AnnotationMetadata",
    "AnnotatorConfidence",
    "EvaluationResult",
    "ExpertAnalysis",
    "GroundTruthDataset",
    "GroundTruthSample",
    "IssueCategory",
    "IssueSeverity",
    "MatchResult",
    "MetricResult",
    "ValidationError",
    "ValidationResult",
    "best_match",
    "completeness",
    "completeness_detailed",
    "count_matches",
    "create_sample",
    "evaluate",
    "evaluate_batch",
    "factual_correctness",
    "factual_correctness_detailed",
    "find_all_matches",
    "fuzzy_match",
    "insight_accuracy",
    "insight_accuracy_detailed",
    "keyword_overlap",
    "load_ground_truth",
    "overall_quality",
    "save_ground_truth",
    "score_accuracy",
    "score_accuracy_detailed",
    "sequence_similarity",
    "validate_dataset",
]
